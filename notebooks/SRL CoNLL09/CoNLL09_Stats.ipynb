{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sent_dict(doc_key, sentence):\n",
    "    global predicate_tag_count\n",
    "    global arg_count\n",
    "    speakers = [['-' for _ in sentence]]\n",
    "    sentences = [[line[1] for line in sentence]]\n",
    "    constituents = [[]]\n",
    "    clusters = []\n",
    "    ner = [[]]\n",
    "    srl = [[]]\n",
    "    predicate_idx = 0\n",
    "    for idx in range(len(sentence)):\n",
    "        if sentence[idx][12] == 'Y': # is predicate\n",
    "            no_arg = True\n",
    "            predicate_tag_count[sentence[idx][4][:2]] += 1\n",
    "            for jdx in range(len(sentence)):\n",
    "                if sentence[jdx][14+predicate_idx] != '_':\n",
    "                    no_arg = False\n",
    "                    srl[0].append([idx, jdx, jdx, sentence[jdx][14+predicate_idx]])\n",
    "                    arg_count[sentence[jdx][14+predicate_idx]] += 1\n",
    "            predicate_idx += 1\n",
    "            if no_arg: # in case of no argument\n",
    "                srl[0].append([idx, idx, idx, '_'])\n",
    "    return {\n",
    "        'speakers':speakers,\n",
    "        'doc_key':doc_key,\n",
    "        'sentences':sentences,\n",
    "        'srl':srl,\n",
    "        'constituents':constituents,\n",
    "        'clusters':clusters,\n",
    "        'ner':ner\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def conll09_to_json(dataset_path, output_path):\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for line in data:\n",
    "        if len(line.strip())>0:\n",
    "            sent.append(line.strip().split('\\t'))\n",
    "        else:\n",
    "            if len(sent)>0:\n",
    "                sentences.append(sent)\n",
    "                sent = []\n",
    "    \n",
    "    if len(sent)>0:\n",
    "        sentences.append(sent)\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        for idx in range(len(sentences)):\n",
    "            json_data = convert_sent_dict('S'+str(idx), sentences[idx])\n",
    "            f.write(json.dumps(json_data)+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/shtoshni/Research/srl/data/conll-2009/conll-2009-p2/data/CoNLL2009-ST-English/CoNLL2009-ST-English-train.txt\"\n",
    "output_path = \"/tmp/output.json\"\n",
    "\n",
    "global predicate_tag_count\n",
    "global arg_count\n",
    "\n",
    "predicate_tag_count = defaultdict(int)\n",
    "arg_count = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll09_to_json(dataset_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'NN': 89780, 'VB': 89193, 'IN': 15, 'RB': 5, 'PR': 12, 'CD': 7, 'TO': 1, 'JJ': 1})\n",
      "\n",
      "[('A1', 146548), ('A0', 99388), ('A2', 46741), ('AM-TMP', 23347), ('AM-MNR', 11837), ('AM-LOC', 10387), ('A3', 9431), ('AM-MOD', 9033), ('AM-ADV', 8541), ('AM-DIS', 4825), ('R-A0', 4039), ('AM-NEG', 3653), ('A4', 3064), ('C-A1', 2754), ('R-A1', 2307), ('AM-PNC', 2233), ('AM-EXT', 1309), ('AM-CAU', 1220), ('AM-DIR', 1146), ('R-AM-TMP', 704), ('R-A2', 286), ('R-AM-LOC', 212), ('R-AM-MNR', 141), ('A5', 82), ('AM-PRD', 69), ('C-A0', 64), ('C-A2', 61), ('R-AM-CAU', 41), ('C-A3', 35), ('R-A3', 28), ('C-AM-MNR', 20), ('C-AM-ADV', 20), ('AM-REC', 14), ('AA', 13), ('R-AM-PNC', 12), ('C-AM-EXT', 11), ('C-A4', 11), ('C-AM-TMP', 10), ('AM', 9), ('C-AM-LOC', 9), ('R-A4', 7), ('C-AM-CAU', 6), ('R-AM-ADV', 5), ('C-AM-PNC', 5), ('C-AM-DIR', 5), ('R-AM-EXT', 4), ('C-AM-DIS', 3), ('AM-PRT', 2), ('AM-TM', 2), ('R-AA', 2), ('C-AM-NEG', 1), ('C-R-AM-TMP', 1), ('R-AM-DIR', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(predicate_tag_count)\n",
    "print()\n",
    "print(sorted(list(arg_count.items()),  key= lambda x: x[1], reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:events] *",
   "language": "python",
   "name": "conda-env-events-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
