{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from os import path\n",
    "import glob\n",
    "from collections import defaultdict, OrderedDict\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "from utils import get_ent_info, get_clusters_from_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(string):\n",
    "    string = string.strip()\n",
    "    if string == \"\":\n",
    "        return []\n",
    "    else:\n",
    "        doc = spacy_nlp(string)\n",
    "        tokenized_sent = [token.text for token in doc if token.text.strip() != '']\n",
    "        return tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(doc_str, ent_list):\n",
    "    \"\"\"Tokenizes a document given in string format.\n",
    "    doc_str: Document string\n",
    "    ent_list: List of entities with each entry being ((span_start, span_end), ent_id) \n",
    "        where spans are provided in the character space.\n",
    "    \n",
    "    Returns:\n",
    "    tokenized_doc: List of tokens\n",
    "    ent_id_to_token_spans: Entity ID to span indices in token space.\n",
    "    \"\"\"\n",
    "    tokenized_doc = []\n",
    "    token_counter = 0  \n",
    "    char_offset = 0  # Till what point has the document been processed\n",
    "    ent_id_to_token_spans = OrderedDict()\n",
    "\n",
    "    for (span_start, span_end), ent_id in ent_list:\n",
    "        # Tokenize the string before the span and after the last span\n",
    "        before_span_str = doc_str[char_offset: span_start]\n",
    "        before_span_tokens = tokenize_string(before_span_str)\n",
    "        tokenized_doc.extend(before_span_tokens)\n",
    "        token_counter += len(before_span_tokens)\n",
    "\n",
    "        # Tokenize the span\n",
    "        span_tokens = tokenize_string(doc_str[span_start: span_end])\n",
    "        ent_id_to_token_spans[ent_id] = (token_counter, token_counter + len(span_tokens))\n",
    "        tokenized_doc.extend(span_tokens)\n",
    "        char_offset = span_end\n",
    "        token_counter += len(span_tokens)\n",
    "\n",
    "    # Add the tokens after the last span\n",
    "    rem_doc = doc_str[char_offset:]\n",
    "    rem_tokens = tokenize_string(rem_doc)\n",
    "    token_counter += len(rem_tokens)\n",
    "\n",
    "    tokenized_doc.extend(rem_tokens)\n",
    "    \n",
    "    return tokenized_doc, ent_id_to_token_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_spans(clusters_ent_id, ent_id_to_token_spans):\n",
    "    clusters = []\n",
    "    for cluster in clusters_ent_id:\n",
    "        cluster_spans = []\n",
    "        for ent_id in cluster:\n",
    "            cluster_spans.append(ent_id_to_token_spans[ent_id])\n",
    "        clusters.append(cluster_spans)\n",
    "        \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_speaker(tokenized_sents):\n",
    "    speakers = []\n",
    "    for sent in tokenized_sents:\n",
    "        speakers.append([\"spk1\"] * len(sent))\n",
    "    return speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_splits_file(list_file):\n",
    "    return set([file_name.strip() for file_name in open(list_file).readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/shtoshni/Research/events/data/red/data/source\"\n",
    "source_files = glob.glob(\"{}/*/*\".format(data_dir))\n",
    "\n",
    "ann_dir = \"/home/shtoshni/Research/events/data/red/data/annotation\"\n",
    "ann_files = glob.glob(\"{}/*/*\".format(ann_dir))\n",
    "\n",
    "# Load the file splits\n",
    "dev_list_file = \"/home/shtoshni/Research/events/data/red/docs/dev.txt\"\n",
    "dev_set = load_splits_file(dev_list_file)\n",
    "\n",
    "test_list_file = \"/home/shtoshni/Research/events/data/red/docs/test.txt\"\n",
    "test_set = load_splits_file(test_list_file)\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"/home/shtoshni/Research/events/data/red/split-ref\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deft/04debcc4da342dc971bdef4210fe468a.mpdf\n",
      "17\n",
      "[['<doc', 'id=\"04debcc4da342dc971bdef4210fe468a\">'], ['<headline>'], ['</headline>'], ['<post', 'author', '='], ['\"', 'Dick', 'here', '\"', 'datetime=\"2008', '-', '01', '-', '11T12:18:00', '\"', 'id=\"p1\">', 'Do', \"n't\", 'order', 'anything', 'online', 'if', 'Amtrak', 'are', 'delivering', 'it', '-', 'here', \"'s\", 'my', 'experience', '.'], ['Ordered', 'a', '32', '\"', 'TV', 'online', ',', 'cheaper', 'than', 'Argos', '-', 'who', 'did', \"n't\", 'have', 'it', 'in', 'stock', '-', 'but', 'with', 'the', 'delivery', 'charge', 'the', 'cost', 'was', 'the', 'same', '.'], ['Advised', 'that', 'it', 'would', 'be', 'delivered', 'by', 'Amtrak', 'on', 'Tuesday', '.'], ['Tuesday', 'came', 'and', 'went', ',', 'no', 'sign', '.'], ['Phoned', 'Amtrak', 'on', 'Wednesday', ',', '\"', 'we', 'need', 'a', 'consignment', 'number', '\"', '.'], ['Phoned', 'online', 'company', 'and', 'got', 'it', '.'], ['Phoned', 'Amtrak', '\"', 'a', 'card', 'was', 'left', 'on', 'Tuesday', 'as', 'you', 'were', \"n't\", 'there', '\"'], ['(', 'no', 'it', 'was', \"n't\", 'of', 'course', ')', ',', 'and'], ['\"', 'we', \"'re\", 'not', 'allowed', 'to', 'leave', 'it', 'with', 'a', 'neighbour', '\"', '.'], ['Arranged', 'for', 'another', 'delivery', 'on', 'Saturday', '.'], ['Arrived', 'home', 'yesterday', '-', 'it', 'had', 'been', 'delivered', 'next', 'door', 'yesterday', ',', 'with', 'a', 'card', 'saying', 'this', 'was', 'their', 'first', 'attempt', 'at', 'delivery', '...'], ['What', 'a', 'bunch', 'of', 'jokers', '.'], ['</post>', '</doc>']]\n"
     ]
    }
   ],
   "source": [
    "# Test ground\n",
    "source_file = \"/home/shtoshni/Research/events/data/red/data/source/deft/04debcc4da342dc971bdef4210fe468a.mpdf\"\n",
    "source_lines = open(source_file).readlines()\n",
    "doc_str = \"\".join(source_lines)\n",
    "doc_str = doc_str.replace('<', '~')\n",
    "doc_str = doc_str.replace('>', '^')\n",
    "\n",
    "\n",
    "# Read the annotation file\n",
    "base_name = path.basename(source_file)\n",
    "dir_name = path.basename(path.dirname(source_file))\n",
    "red_file_name = path.join(dir_name, base_name)\n",
    "print(red_file_name)\n",
    "\n",
    "ann_file = path.join(path.join(ann_dir, dir_name), base_name + \".RED-Relation.gold.completed.xml\")    \n",
    "tree = ET.parse(ann_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Get entity and cluster information from the annotation file\n",
    "ent_map, ent_list = get_ent_info(root)\n",
    "clusters_ent_id = get_clusters_from_xml(root, ent_map)\n",
    "\n",
    "# Tokenize the doc\n",
    "tokenized_doc, ent_id_to_token_spans = tokenize_doc(doc_str, ent_list)\n",
    "\n",
    "# Break the document into sentences.\n",
    "tokenized_sents = []\n",
    "tokenized_doc_str = \" \".join(tokenized_doc)\n",
    "reproc_doc = spacy_nlp(tokenized_doc_str)\n",
    "for sent in reproc_doc.sents:\n",
    "    sent_text = sent.text\n",
    "    sent_text = sent_text.replace('~', '<')\n",
    "    sent_text = sent_text.replace('^', '>')\n",
    "    tokenized_sents.append(sent_text.split())\n",
    "    \n",
    "    \n",
    "print(len(tokenized_sents))\n",
    "print(tokenized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deft/NYT_ENG_20130426.0143\n",
      "deft/NYT_ENG_20131225.0200\n",
      "deft/NYT_ENG_20130525.0040\n",
      "deft/APW_ENG_20101231.0037\n",
      "deft/7677d625b58ce649c8aeda2ff4a56389.mpdf\n",
      "deft/NYT_ENG_20131029.0091\n",
      "deft/NYT_ENG_20131003.0269\n",
      "deft/362f9d9707c4da0c8068bc7034aae4b4.mpdf\n",
      "deft/NYT_ENG_20130613.0153\n",
      "deft/565fa81d640f451b20955887a43b3a23.mpdf\n",
      "deft/aa003ea934a97bac86cee52b7122f1f8.mpdf\n",
      "deft/5e3fbf49f8301654bb4954c0f1e386a9.mpdf\n",
      "deft/NYT_ENG_20130619.0092\n",
      "deft/17a2dc40635ec239e9e16d10b6dd45e8.mpdf\n",
      "deft/AFP_ENG_20100414.0615\n",
      "deft/d4698e3ad06f896058ade2e8f3a09577.mpdf\n",
      "deft/dd0b65f632f64369c530f9bbb4b024b4.mpdf\n",
      "deft/5c7ea2b51202d80ee37eba8a182afad3.mpdf\n",
      "deft/NYT_ENG_20130703.0214\n",
      "deft/2d2a4ddb1c8f4a669541704f9fb78472.mpdf\n",
      "deft/NYT_ENG_20131128.0177\n",
      "deft/635bde2afdaaf20a0bcdc3b5f79578c9.mpdf\n",
      "deft/4798bc0e166fe93893bdf2d922f06258.mpdf\n",
      "deft/648abb9000309b9807cc8b212c11254f.mpdf\n",
      "deft/37b56b6dd846ad0dd6e8cd00ba2efaf4.mpdf\n",
      "deft/af18d29036ab0a9f8cf2742a5a1b4804.mpdf\n",
      "deft/ca2a6fbf721ca102c149ad6a90d5b00a.mpdf\n",
      "deft/1d2911e09a6746b942c3e7b3cbdcb0ce.mpdf\n",
      "deft/d6bc66d7c8423368aaa8d789b5bdf5db.mpdf\n",
      "deft/XIN_ENG_20101125.0137\n",
      "deft/NYT_ENG_20130822.0136\n",
      "deft/4829d3d91263ed9d8801e6d94c3569a5.mpdf\n",
      "deft/NYT_ENG_20131022.0102\n",
      "deft/NYT_ENG_20130509.0160\n",
      "deft/NYT_ENG_20130424.0047\n",
      "deft/NYT_ENG_20131029.0042\n",
      "deft/5c0dd992beaff240f732e0fdacbd49e4.mpdf\n",
      "deft/04debcc4da342dc971bdef4210fe468a.mpdf\n",
      "deft/NYT_ENG_20130710.0155\n",
      "deft/5c59566e9132c060423cad5b2d1bac1e.mpdf\n",
      "deft/d7369ce92ed0b6327412c705dbbab654.mpdf\n",
      "deft/NYT_ENG_20130828.0147\n",
      "deft/NYT_ENG_20130816.0151\n",
      "deft/NYT_ENG_20130625.0044\n",
      "deft/NYT_ENG_20131115.0084\n",
      "deft/NYT_ENG_20131029.0228\n",
      "deft/NYT_ENG_20130712.0047\n",
      "deft/NYT_ENG_20130603.0111\n",
      "deft/96bf72399b104346f3e79022e0c08e5a.mpdf\n",
      "deft/c06e8bbdf69f73a69cd3d5dbb4d06a21.mpdf\n",
      "deft/NYT_ENG_20131220.0283\n",
      "deft/APW_ENG_20090611.0697\n",
      "deft/AFP_ENG_20100601.0724\n",
      "deft/NYT_ENG_20130506.0130\n",
      "deft/NYT_ENG_20130709.0087\n",
      "pilot/rec.games.chess.politics_20041217.2111\n",
      "pilot/c572dc8353079a5ef9943ae298e4f5b0\n",
      "pilot/90e2a980c22e41e2b25666f676458343\n",
      "pilot/29f64df7feb04dfb16f4667ce199c9f0\n",
      "pilot/be8792abc57847d9a19940579acf8373\n",
      "pilot/57026b7bcb8f855de3e26d572db35285\n",
      "pilot/soc.culture.iraq_20050211.0445\n",
      "pilot/rec.arts.sf.written.robert-jordan_20050208.1350\n",
      "pilot/7ddae34c4553ef60a76c43144855ec6b\n",
      "pilot/uk.gay-lesbian-bi_20050127.0311\n",
      "pilot/alt.sys.pc-clone.dell_20050226.2350\n",
      "pilot/1b268b27094ba9c5feb11192dad940ab\n",
      "pilot/soc.culture.china_20050203.0639\n",
      "pilot/misc.legal.moderated_20050129.2225\n",
      "pilot/44b011cd504c9ed71beb851324db886a\n",
      "pilot/d21dc2cb6e6435da7f9d9b0e5759e214\n",
      "pilot/misc.legal.moderated_20041202.1648\n",
      "pilot/alt.support.divorce_20050113.2451\n",
      "pilot/soc.org.nonprofit_20050218.1902\n",
      "pilot/0f03cc5a508d630c6c8c8c61396e31a9\n",
      "proxy/PROXY_AFP_ENG_20020123_0346\n",
      "proxy/PROXY_AFP_ENG_20020126_0161\n",
      "proxy/PROXY_AFP_ENG_20020115_0320\n",
      "proxy/PROXY_AFP_ENG_20020301_0316\n",
      "proxy/PROXY_AFP_ENG_20020414_0542\n",
      "proxy/PROXY_AFP_ENG_20020201_0053\n",
      "proxy/PROXY_AFP_ENG_20020416_0331\n",
      "proxy/PROXY_AFP_ENG_20020123_0334\n",
      "proxy/PROXY_AFP_ENG_20020406_0538\n",
      "proxy/PROXY_AFP_ENG_20020404_0305\n",
      "proxy/PROXY_AFP_ENG_20020117_0270\n",
      "proxy/PROXY_AFP_ENG_20020210_0074\n",
      "proxy/PROXY_AFP_ENG_20020408_0307\n",
      "proxy/PROXY_AFP_ENG_20020128_0449\n",
      "proxy/PROXY_AFP_ENG_20020105_0162\n",
      "proxy/PROXY_AFP_ENG_20020106_0520\n",
      "proxy/PROXY_AFP_ENG_20020408_0348\n",
      "proxy/PROXY_AFP_ENG_20020111_0093\n",
      "proxy/PROXY_AFP_ENG_20020304_0028\n",
      "proxy/PROXY_AFP_ENG_20020329_0022\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "dev_data = []\n",
    "test_data = []\n",
    "\n",
    "for source_file in source_files:\n",
    "    # Read the source doc\n",
    "    source_lines = open(source_file).readlines()\n",
    "    doc_str = \"\".join(source_lines)\n",
    "    \n",
    "    # ADDED NEW RULES\n",
    "    doc_str = doc_str.replace('<', '~')\n",
    "    doc_str = doc_str.replace('>', '^')\n",
    "    \n",
    "    # Read the annotation file\n",
    "    base_name = path.basename(source_file)\n",
    "    dir_name = path.basename(path.dirname(source_file))\n",
    "    red_file_name = path.join(dir_name, base_name)\n",
    "    print(red_file_name)\n",
    "    \n",
    "    ann_file = path.join(path.join(ann_dir, dir_name), base_name + \".RED-Relation.gold.completed.xml\")    \n",
    "    tree = ET.parse(ann_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Get entity and cluster information from the annotation file\n",
    "    ent_map, ent_list = get_ent_info(root)\n",
    "    clusters_ent_id = get_clusters_from_xml(root, ent_map)\n",
    "    \n",
    "    # Tokenize the doc\n",
    "    tokenized_doc, ent_id_to_token_spans = tokenize_doc(doc_str, ent_list)\n",
    "    \n",
    "    # Break the document into sentences.\n",
    "    tokenized_sents = []\n",
    "    tokenized_doc_str = \" \".join(tokenized_doc)\n",
    "    reproc_doc = spacy_nlp(tokenized_doc_str)\n",
    "    \n",
    "    # SWITCH BACK TO THE ORIGINAL TOKENS\n",
    "    tokenized_doc_str = tokenized_doc_str.replace('~', '<')\n",
    "    tokenized_doc_str = tokenized_doc_str.replace('^', '>')\n",
    "    tokenized_doc = tokenized_doc_str.split()\n",
    "    \n",
    "    for sent in reproc_doc.sents:\n",
    "        sent_text = sent.text\n",
    "        # SWITCH BACK TO THE ORIGINAL TOKENS\n",
    "        sent_text = sent_text.replace('~', '<')\n",
    "        sent_text = sent_text.replace('^', '>')\n",
    "        tokenized_sents.append(sent_text.split())\n",
    "#         tokenized_sents.append(sent_text)\n",
    "\n",
    "    cluster_spans = get_cluster_spans(clusters_ent_id, ent_id_to_token_spans)\n",
    "    \n",
    "    try:\n",
    "        # Check the retokenized doc is same as tokenized doc\n",
    "        retokenized_doc = []\n",
    "        for sent in tokenized_sents:\n",
    "            retokenized_doc.extend(sent)\n",
    "        assert(tokenized_doc == retokenized_doc)\n",
    "    except AssertionError:\n",
    "        print(len(tokenized_doc))\n",
    "        print(len(retokenized_doc))\n",
    "        break\n",
    "\n",
    "    doc_info = {}\n",
    "    doc_info[\"doc_key\"] = red_file_name\n",
    "    doc_info[\"sentences\"] = tokenized_sents\n",
    "    doc_info[\"clusters\"] = cluster_spans\n",
    "    doc_info[\"speakers\"] = get_dummy_speaker(tokenized_sents)\n",
    "    \n",
    "    file_name = path.join(dir_name, base_name)\n",
    "    if red_file_name in dev_set:\n",
    "        dev_data.append(doc_info)\n",
    "    elif red_file_name in test_set:\n",
    "        test_data.append(doc_info)\n",
    "    else:\n",
    "        train_data.append(doc_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, data in zip(['train', 'dev', 'test'], [train_data, dev_data, test_data]):\n",
    "    with open(path.join(output_dir, \"{}.english.jsonlines\".format(split)), 'w') as f:\n",
    "        for instance in data:\n",
    "            f.write(json.dumps(instance) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stats on train data\n",
    "max_num_sentences = 0\n",
    "num_chains = []\n",
    "span_length = []\n",
    "chain_length = []\n",
    "\n",
    "for instance in train_data:\n",
    "    max_num_sentences = max(max_num_sentences, len(instance[\"sentences\"]))\n",
    "    num_chains.append(len(instance[\"clusters\"]))\n",
    "    for chain in instance[\"clusters\"]:\n",
    "        chain_length.append(len(chain))\n",
    "        for mention in chain:\n",
    "            span_start, span_end = mention\n",
    "            span_length.append(span_end - span_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentences: 258\n",
      "Max chains: 60\n",
      "Max span: 9\n",
      "Max chain length: 71\n"
     ]
    }
   ],
   "source": [
    "print(\"Max sentences:\", max_num_sentences)\n",
    "print(\"Max chains:\", max(num_chains))\n",
    "print(\"Max span:\", max(span_length))\n",
    "print(\"Max chain length:\", max(chain_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([196.,   7.,   3.,   5.,   3.,   1.,   0.,   1.,   0.,   1.]),\n",
       " array([ 2. ,  6.3, 10.6, 14.9, 19.2, 23.5, 27.8, 32.1, 36.4, 40.7, 45. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(chain_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:narrative_10]",
   "language": "python",
   "name": "conda-env-narrative_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
