{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from os import path\n",
    "import glob\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "from utils import get_ent_info, get_clusters_from_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/shtoshni/Research/events/data/red/data/source\"\n",
    "source_files = glob.glob(\"{}/*/*\".format(data_dir))\n",
    "\n",
    "ann_dir = \"/home/shtoshni/Research/events/data/red/data/annotation\"\n",
    "ann_files = glob.glob(\"{}/*/*\".format(ann_dir))\n",
    "\n",
    "output_dir = \"/home/shtoshni/Research/events/data/red/error_logs\"\n",
    "if not path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Types of Coref Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporalRelations\n",
      "\tTLINK\t4209\n",
      "\tALINK\t136\n",
      "\n",
      "CorefChains\n",
      "\tIDENTICAL\t2049\n",
      "\tWHOLE/PART\t515\n",
      "\tSET/MEMBER\t947\n",
      "\tAPPOSITIVE\t303\n",
      "\tBRIDGING\t383\n",
      "\tREPORTING\t596\n",
      "\n",
      "\n",
      "\n",
      "TLINK\tSource\tType\tTarget\tPolarity\tContextualModality\tDifficulty\n",
      "\t4209\t4209\t4209\t4209\t4209\t4209\n",
      "IDENTICAL\tFirstInstance\tCoreferring_String\n",
      "\t2049\t6512\n",
      "WHOLE/PART\tWhole\tPart\n",
      "\t515\t815\n",
      "SET/MEMBER\tSet\tMember\n",
      "\t947\t1614\n",
      "ALINK\tSource\tType\tTarget\tDifficulty\n",
      "\t136\t136\t136\t136\n",
      "APPOSITIVE\tHead\tAttribute\n",
      "\t303\t303\n",
      "BRIDGING\tArgument\tRelated_to\tFirstInstance\tCoreferring_String\n",
      "\t381\t433\t2\t2\n",
      "REPORTING\tReport\tEvent\n",
      "\t596\t1334\n"
     ]
    }
   ],
   "source": [
    "parents_type = defaultdict(Counter)\n",
    "type_properties = defaultdict(Counter)\n",
    "\n",
    "for source_file in source_files:\n",
    "    # Read the source doc\n",
    "    source_lines = open(source_file).readlines()\n",
    "    source_str = \"\".join(source_lines)\n",
    "    \n",
    "    # Read the annotation file\n",
    "    base_name = path.basename(source_file)\n",
    "    dir_name = path.basename(path.dirname(source_file))\n",
    "    \n",
    "    ann_file = path.join(path.join(ann_dir, dir_name), base_name + \".RED-Relation.gold.completed.xml\")\n",
    "    \n",
    "    tree = ET.parse(ann_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    \n",
    "    for elem in root.iter('relation'):\n",
    "        parenttype_elem = elem.find('parentsType')\n",
    "        type_elem = elem.find('type')\n",
    "        parents_type[parenttype_elem.text][type_elem.text]+= 1\n",
    "        \n",
    "        prop_elem = elem.find('properties')\n",
    "        for tag_elem in prop_elem:\n",
    "            type_properties[type_elem.text][tag_elem.tag] += 1 \n",
    "        \n",
    "\n",
    "for parent_type in parents_type:\n",
    "    print(parent_type)\n",
    "    for subtype in parents_type[parent_type]:\n",
    "        print(\"\\t{}\\t{}\".format(subtype, parents_type[parent_type][subtype])) \n",
    "    print()\n",
    "print(\"\\n\")\n",
    "\n",
    "for elem_type in type_properties:\n",
    "    attribute_counts = type_properties[elem_type]\n",
    "    attribs, attrib_counts = [], []\n",
    "    for attrib in attribute_counts:\n",
    "        attribs.append(attrib)\n",
    "        attrib_counts.append(attribute_counts[attrib])\n",
    "        \n",
    "    print(elem_type, *attribs, sep=\"\\t\")\n",
    "    attrib_counts = [''] + attrib_counts\n",
    "    print(*attrib_counts, sep=\"\\t\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search For Problematic Spans Marked as Multiple Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files with issues: 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files_with_issues = defaultdict(list)\n",
    "\n",
    "\n",
    "for source_file in source_files:\n",
    "    # Read the source doc\n",
    "    source_lines = open(source_file).readlines()\n",
    "    source_str = \"\".join(source_lines)\n",
    "    \n",
    "    # Read the annotation file\n",
    "    base_name = path.basename(source_file)\n",
    "    dir_name = path.basename(path.dirname(source_file))\n",
    "    \n",
    "    ann_file = path.join(path.join(ann_dir, dir_name), base_name + \".RED-Relation.gold.completed.xml\")\n",
    "    \n",
    "    tree = ET.parse(ann_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    span_to_elem_id = {}\n",
    "    # Get info from the XML file\n",
    "    for elem in root.iter('entity'):\n",
    "        span_str = list(elem.iter('span'))[0].text\n",
    "        span_start, span_end = [int(endpoint) for endpoint in span_str.split(\",\")]\n",
    "        elem_id = list(elem.iter('id'))[0].text\n",
    "        elem_type = list(elem.iter('type'))[0].text\n",
    "        \n",
    "        span = tuple((span_start, span_end))\n",
    "        if elem_type == 'ENTITY' or elem_type == 'EVENT':\n",
    "            if span in span_to_elem_id:\n",
    "#                 if elem_id == span_to_elem_id[span]:\n",
    "#                     continue\n",
    "#                 else:\n",
    "                files_with_issues[dir_name + \"-\" + base_name].append(\n",
    "                    ('Multiple Entities', span, source_str[span_start: span_end],\n",
    "                     source_str[span_start-15: span_end+15].replace(\"\\n\", \" \"),\n",
    "                     span_to_elem_id[span][0], span_to_elem_id[span][1], \n",
    "                     elem_type, elem_id))\n",
    "            else:\n",
    "                span_to_elem_id[span] = (elem_type, elem_id)\n",
    "            \n",
    "            \n",
    "file_issues_list = list(files_with_issues.items())\n",
    "file_issues_list = sorted(file_issues_list, key=lambda x: x[1], reverse=True)\n",
    "print(\"Total files with issues: {}\\n\".format(len(files_with_issues)))\n",
    "# for file_name, num_issues in file_issues_list:\n",
    "#     print(file_name, num_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search For Spans That Are Part of Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files with issues: 41\n",
      "\n",
      "Check /home/shtoshni/Research/events/data/red/error_logs for issues with each file\n"
     ]
    }
   ],
   "source": [
    "# files_with_issues = defaultdict(int)\n",
    "\n",
    "for source_file in source_files:\n",
    "    # Read the source doc\n",
    "    source_lines = open(source_file).readlines()\n",
    "    source_str = \"\".join(source_lines)\n",
    "    \n",
    "    # Read the annotation file\n",
    "    base_name = path.basename(source_file)\n",
    "    dir_name = path.basename(path.dirname(source_file))\n",
    "    \n",
    "    ann_file = path.join(path.join(ann_dir, dir_name), base_name + \".RED-Relation.gold.completed.xml\")\n",
    "    \n",
    "    tree = ET.parse(ann_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # First build the element ID to span mapping because element IDs are used in CorefChains\n",
    "    elem_id_to_span = {}\n",
    "    for elem in root.iter('entity'):\n",
    "        span_str = list(elem.iter('span'))[0].text\n",
    "        span_start, span_end = [int(endpoint) for endpoint in span_str.split(\",\")]\n",
    "        elem_id = list(elem.iter('id'))[0].text\n",
    "        elem_type = list(elem.iter('type'))[0].text\n",
    "        \n",
    "        span = tuple((span_start, span_end))\n",
    "        if elem_type == 'ENTITY' or elem_type == 'EVENT':\n",
    "            if elem_id in elem_id_to_span:\n",
    "                if span == elem_id_to_span[elem_id]:\n",
    "                    continue\n",
    "                else:\n",
    "                    print(elem_id, span_to_elem_id[span])\n",
    "            else:\n",
    "                elem_id_to_span[elem_id] = span\n",
    "                \n",
    "    \n",
    "    span_to_cluster = {}\n",
    "    cluster_cnter = 0\n",
    "    for elem in root.iter('relation'):\n",
    "        type_elem = elem.find('type').text\n",
    "        if type_elem == 'IDENTICAL':\n",
    "            cluster_id = elem.find('id').text\n",
    "            prop_elem = elem.find('properties')\n",
    "            for sub_elem in prop_elem:\n",
    "                ent_id = sub_elem.text\n",
    "                span = elem_id_to_span[ent_id]\n",
    "                \n",
    "                if span in span_to_cluster:\n",
    "                    span_start, span_end = span\n",
    "                    files_with_issues[dir_name + \"-\" + base_name].append(\n",
    "                        ('Multiple Clusters', source_str[span_start: span_end], \n",
    "                         source_str[span_start-15: span_end+15].replace(\"\\n\", \" \"), span, ent_id, \n",
    "                         span_to_cluster[span], cluster_id))\n",
    "                else:\n",
    "                    span_to_cluster[span] = cluster_id\n",
    "                \n",
    "            cluster_cnter += 1\n",
    "\n",
    "file_issues_list = list(files_with_issues.items())\n",
    "file_issues_list = sorted(file_issues_list, key=lambda x: x[1], reverse=True)\n",
    "print(\"Total files with issues: {}\\n\".format(len(files_with_issues)))\n",
    "print(\"Check {} for issues with each file\".format(output_dir))\n",
    "for file_name, issues in file_issues_list:\n",
    "    with open(path.join(output_dir, file_name + \".txt\"), 'w') as f:\n",
    "        f.write(\"Number of issues: {}\\n\\n\".format(len(issues)))\n",
    "        for issue in issues:\n",
    "            issue_str = [str(issue_attrib) for issue_attrib in issue]\n",
    "            f.write(\"   \".join(issue_str) + \"\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num issues: 1, Num files: 17\n",
      "Num issues: 2, Num files: 10\n",
      "Num issues: 3, Num files: 3\n",
      "Num issues: 4, Num files: 2\n",
      "Num issues: 5, Num files: 2\n",
      "Num issues: 6, Num files: 2\n",
      "Num issues: 8, Num files: 1\n",
      "Num issues: 9, Num files: 1\n",
      "Num issues: 10, Num files: 1\n",
      "Num issues: 44, Num files: 1\n",
      "Num issues: 60, Num files: 1\n"
     ]
    }
   ],
   "source": [
    "num_issues_to_num_files = defaultdict(list)\n",
    "for file_name, issues in file_issues_list:\n",
    "    num_issues_to_num_files[len(issues)].append(file_name)\n",
    "    \n",
    "num_issues = sorted(num_issues_to_num_files.keys())\n",
    "with open(path.join(output_dir, \"meta.txt\"), 'w') as f:\n",
    "    for num_issue in num_issues:\n",
    "        f.write(\"Num issues:{}\\n\".format(num_issue))\n",
    "        print(\"Num issues: {}, Num files: {}\".format(num_issue, len(num_issues_to_num_files[num_issue])))\n",
    "        for file_name in num_issues_to_num_files[num_issue]:\n",
    "            f.write(file_name + \"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:narrative_10]",
   "language": "python",
   "name": "conda-env-narrative_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
