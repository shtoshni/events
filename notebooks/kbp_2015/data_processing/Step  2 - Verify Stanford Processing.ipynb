{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from os import path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/shtoshni/Research/events/src\")\n",
    "\n",
    "from kbp_2015_utils.constants import SPLIT_TO_DIR, SUBDIR_DICT, SUBDIR_EXT\n",
    "from data_processing.kbp_2015.utils import parse_ann_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input_dir = \"/home/shtoshni/Research/events/data/kbp_2014-2015/data/2015\"\n",
    "output_dir = \"/home/shtoshni/Research/events/proc_data/kbp_2015/remove_xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load tokens in StanfordNLP processed JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens_in_json(json_file):\n",
    "#     test_file = \"/home/shtoshni/Research/events/proc_data/kbp_2015/remove_xml/3f71fead3fa119ccdcdf01769ffee5b1.txt.json\"\n",
    "    data = json.loads(open(json_file).read())\n",
    "    sentences = data[\"sentences\"]\n",
    "#     print(len(sentences))\n",
    "\n",
    "    token_idx_to_char = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence[\"tokens\"]\n",
    "        speaker_set = set()\n",
    "        for token in tokens:\n",
    "            if \"speaker\" in token:\n",
    "                speaker_set.add(token[\"speaker\"])\n",
    "            \n",
    "            offset = token[\"characterOffsetBegin\"]\n",
    "            token_text = token[\"originalText\"]\n",
    "            for token_idx in range(token[\"characterOffsetBegin\"], token[\"characterOffsetEnd\"]):\n",
    "                token_idx_to_char[token_idx] = token_text[token_idx - offset]\n",
    "        \n",
    "        assert (len(speaker_set) <= 1)\n",
    "        \n",
    "    return token_idx_to_char\n",
    "#     print(len(token_start_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify files\n",
    "\n",
    "#### Verification reveals that only 3 of the annotations marked in URLs are ignored in preprocessing. That's fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562 568 raph.com.au/news/national/men-charded-over-sharia-law-lashing/story /home/shtoshni/Research/events/data/kbp_2014-2015/data/2015/mod_training/source/bolt-eng-DF-170-181109-47916.txt\n",
      "586 592 l/men-charded-over-sharia-law-lashing/story-e6freuzr-1226097641832\" /home/shtoshni/Research/events/data/kbp_2014-2015/data/2015/mod_training/source/bolt-eng-DF-170-181109-47916.txt\n",
      "242 248 /2010/12/nigerias-drop-cheney-charges-illegal/\">Nigeriaâ€™s deal to d /home/shtoshni/Research/events/data/kbp_2014-2015/data/2015/mod_dev/source/bolt-eng-DF-170-181109-48534.txt\n"
     ]
    }
   ],
   "source": [
    "def verify_partition(split):\n",
    "    split_dir = path.join(base_input_dir, SPLIT_TO_DIR[split])\n",
    "    source_dir = path.join(split_dir, SUBDIR_DICT[\"source\"])\n",
    "    ann_dir = path.join(split_dir, SUBDIR_DICT[\"ann\"])\n",
    "\n",
    "    source_files = sorted(glob.glob(path.join(source_dir, \"*\" + SUBDIR_EXT[\"source\"])))\n",
    "    \n",
    "    proc_source_files, ann_files = [], []\n",
    "    for source_file in source_files:\n",
    "        source_text = open(source_file).read()\n",
    "        doc_id = path.splitext(path.basename(source_file))[0]\n",
    "        ann_file = path.join(ann_dir, doc_id + SUBDIR_EXT[\"ann\"])\n",
    "        proc_source_file = path.join(output_dir, path.basename(source_file) + \".json\")            \n",
    "        assert (path.exists(ann_file))\n",
    "        assert (path.exists(proc_source_file))\n",
    "        \n",
    "        ann_files.append(ann_file)\n",
    "        proc_source_files.append(proc_source_file)\n",
    "        \n",
    "        token_idx_to_char = load_tokens_in_json(proc_source_file)\n",
    "        mention_list = parse_ann_file(ann_file)[1]\n",
    "        \n",
    "        for (span_start, span_end, _) in mention_list:\n",
    "            if (span_start not in token_idx_to_char) or ((span_end - 1) not in token_idx_to_char):\n",
    "                print(span_start, span_end -1, source_text[span_start-30:span_end+30], source_file,)\n",
    "               \n",
    "#     print(proc_source_files[0], ann_files[0])\n",
    "\n",
    "for split in SPLIT_TO_DIR:\n",
    "    verify_partition(split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:events] *",
   "language": "python",
   "name": "conda-env-events-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
