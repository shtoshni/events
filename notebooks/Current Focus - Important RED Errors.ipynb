{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from os import path\n",
    "import glob\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "from utils import get_ent_info, get_clusters_from_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/shtoshni/Research/events/data/red/data/source\"\n",
    "source_files = glob.glob(\"{}/*/*\".format(data_dir))\n",
    "\n",
    "ann_dir = \"/home/shtoshni/Research/events/data/red/data/mod_annotation\"\n",
    "ann_files = glob.glob(\"{}/*/*\".format(ann_dir))\n",
    "\n",
    "output_dir = \"/home/shtoshni/Research/events/data/red/imp_error_logs\"\n",
    "if not path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search For Problematic Spans Marked as Multiple Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files with issues: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files_with_issues = defaultdict(list)\n",
    "\n",
    "\n",
    "for source_file in source_files:\n",
    "    # Read the source doc\n",
    "    source_lines = open(source_file).readlines()\n",
    "    source_str = \"\".join(source_lines)\n",
    "    \n",
    "    # Read the annotation file\n",
    "    base_name = path.basename(source_file)\n",
    "    dir_name = path.basename(path.dirname(source_file))\n",
    "    \n",
    "    ann_file = path.join(path.join(ann_dir, dir_name), base_name + \".RED-Relation.gold.completed.xml\")\n",
    "    \n",
    "    tree = ET.parse(ann_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    span_to_elem_id = {}\n",
    "    # Get info from the XML file\n",
    "    for elem in root.iter('entity'):\n",
    "        span_str = list(elem.iter('span'))[0].text\n",
    "        span_start, span_end = [int(endpoint) for endpoint in span_str.split(\",\")]\n",
    "        elem_id = list(elem.iter('id'))[0].text\n",
    "        elem_type = list(elem.iter('type'))[0].text\n",
    "        \n",
    "        span = tuple((span_start, span_end))\n",
    "        if elem_type == 'ENTITY' or elem_type == 'EVENT':\n",
    "            if (span in span_to_elem_id):\n",
    "                same_something = False\n",
    "                for (prev_elem_type, prev_elem_id) in span_to_elem_id[span]:\n",
    "                    if prev_elem_type == elem_type or prev_elem_id == elem_id:\n",
    "                        same_something = True\n",
    "                        files_with_issues[dir_name + \"-\" + base_name].append(\n",
    "                            ('Multiple Entities', span, source_str[span_start: span_end],\n",
    "                             source_str[span_start-15: span_end+15].replace(\"\\n\", \" \"),\n",
    "                             span_to_elem_id[span][0], span_to_elem_id[span][1], \n",
    "                             elem_type, elem_id))\n",
    "                if not same_something:\n",
    "                    span_to_elem_id[span].append((elem_type, elem_id))\n",
    "            else:\n",
    "                span_to_elem_id[span]= [(elem_type, elem_id)]\n",
    "            \n",
    "            \n",
    "file_issues_list = list(files_with_issues.items())\n",
    "file_issues_list = sorted(file_issues_list, key=lambda x: x[1], reverse=True)\n",
    "print(\"Total files with issues: {}\\n\".format(len(files_with_issues)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search For ElementsThat Are Part of Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_file in source_files:\n",
    "    # Read the source doc\n",
    "    source_lines = open(source_file).readlines()\n",
    "    source_str = \"\".join(source_lines)\n",
    "    \n",
    "    # Read the annotation file\n",
    "    base_name = path.basename(source_file)\n",
    "    dir_name = path.basename(path.dirname(source_file))\n",
    "    \n",
    "    ann_file = path.join(path.join(ann_dir, dir_name), base_name + \".RED-Relation.gold.completed.xml\")\n",
    "    \n",
    "    tree = ET.parse(ann_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # First build the element ID to span mapping because element IDs are used in CorefChains\n",
    "    elem_id_to_span = {}\n",
    "    for elem in root.iter('entity'):\n",
    "        span_str = list(elem.iter('span'))[0].text\n",
    "        span_start, span_end = [int(endpoint) for endpoint in span_str.split(\",\")]\n",
    "        elem_id = list(elem.iter('id'))[0].text\n",
    "        elem_type = list(elem.iter('type'))[0].text\n",
    "        \n",
    "        span = tuple((span_start, span_end))\n",
    "        if elem_type == 'ENTITY' or elem_type == 'EVENT':\n",
    "            if elem_id in elem_id_to_span:\n",
    "                if span == elem_id_to_span[elem_id]:\n",
    "                    continue\n",
    "                else:\n",
    "                    print(elem_id, span_to_elem_id[span])\n",
    "            else:\n",
    "                elem_id_to_span[elem_id] = span\n",
    "                \n",
    "    \n",
    "    ent_to_cluster = {}\n",
    "    cluster_cnter = 0\n",
    "    for elem in root.iter('relation'):\n",
    "        type_elem = elem.find('type').text\n",
    "        if type_elem == 'IDENTICAL':\n",
    "            cluster_id = elem.find('id').text\n",
    "            prop_elem = elem.find('properties')\n",
    "            for sub_elem in prop_elem:\n",
    "                ent_id = sub_elem.text\n",
    "                span = elem_id_to_span[ent_id]\n",
    "                \n",
    "                if ent_id in ent_to_cluster:\n",
    "                    span_start, span_end = span\n",
    "                    files_with_issues[dir_name + \"-\" + base_name].append(\n",
    "                        ('Multiple Clusters', source_str[span_start: span_end], \n",
    "                         source_str[span_start-15: span_end+15].replace(\"\\n\", \" \"), span, ent_id, \n",
    "                         ent_to_cluster[ent_id], cluster_id))\n",
    "                else:\n",
    "                    ent_to_cluster[ent_id] = cluster_id\n",
    "                \n",
    "            cluster_cnter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for chains that don't have elements of the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_file in source_files:\n",
    "    # Read the source doc\n",
    "    source_lines = open(source_file).readlines()\n",
    "    source_str = \"\".join(source_lines)\n",
    "    \n",
    "    # Read the annotation file\n",
    "    base_name = path.basename(source_file)\n",
    "    dir_name = path.basename(path.dirname(source_file))\n",
    "    \n",
    "    ann_file = path.join(path.join(ann_dir, dir_name), base_name + \".RED-Relation.gold.completed.xml\")\n",
    "    \n",
    "    tree = ET.parse(ann_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # First build the element ID to span mapping because element IDs are used in CorefChains\n",
    "    elem_id_to_type = {}\n",
    "    for elem in root.iter('entity'):\n",
    "        span_str = list(elem.iter('span'))[0].text\n",
    "        span_start, span_end = [int(endpoint) for endpoint in span_str.split(\",\")]\n",
    "        elem_id = list(elem.iter('id'))[0].text\n",
    "        elem_type = list(elem.iter('type'))[0].text\n",
    "        \n",
    "        span = tuple((span_start, span_end))\n",
    "        if elem_type == 'ENTITY' or elem_type == 'EVENT':\n",
    "            elem_id_to_type[elem_id] = elem_type\n",
    "                \n",
    "    \n",
    "    for elem in root.iter('relation'):\n",
    "        type_elem = elem.find('type').text\n",
    "        if type_elem == 'IDENTICAL':\n",
    "            cluster_id = elem.find('id').text\n",
    "            prop_elem = elem.find('properties')\n",
    "            cluster_type = set()\n",
    "            for sub_elem in prop_elem:\n",
    "                ent_id = sub_elem.text\n",
    "                cluster_type.add(elem_id_to_type[ent_id])\n",
    "                \n",
    "            if len(cluster_type) > 1:\n",
    "                files_with_issues[dir_name + \"-\" + base_name].append(\n",
    "                        ('Inconsistent Type', cluster_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files with issues: 0\n",
      "\n",
      "Check /home/shtoshni/Research/events/data/red/imp_error_logs for issues with each file\n"
     ]
    }
   ],
   "source": [
    "file_issues_list = list(files_with_issues.items())\n",
    "file_issues_list = sorted(file_issues_list, key=lambda x: x[1], reverse=True)\n",
    "print(\"Total files with issues: {}\\n\".format(len(files_with_issues)))\n",
    "print(\"Check {} for issues with each file\".format(output_dir))\n",
    "for file_name, issues in file_issues_list:\n",
    "    with open(path.join(output_dir, file_name + \".txt\"), 'w') as f:\n",
    "        f.write(\"Number of issues: {}\\n\\n\".format(len(issues)))\n",
    "        for issue in issues:\n",
    "            issue_str = [str(issue_attrib) for issue_attrib in issue]\n",
    "            f.write(\"   \".join(issue_str) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_issues_to_num_files = defaultdict(list)\n",
    "for file_name, issues in file_issues_list:\n",
    "    num_issues_to_num_files[len(issues)].append(file_name)\n",
    "    \n",
    "num_issues = sorted(num_issues_to_num_files.keys())\n",
    "with open(path.join(output_dir, \"meta.txt\"), 'w') as f:\n",
    "    for num_issue in num_issues:\n",
    "        f.write(\"Num issues:{}\\n\".format(num_issue))\n",
    "        print(\"Num issues: {}, Num files: {}\".format(num_issue, len(num_issues_to_num_files[num_issue])))\n",
    "        for file_name in num_issues_to_num_files[num_issue]:\n",
    "            f.write(file_name + \"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:narrative_10]",
   "language": "python",
   "name": "conda-env-narrative_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
